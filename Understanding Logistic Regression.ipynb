{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vermadev54/Machine-learning-basic-algorithm/blob/master/Understanding%20Logistic%20Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TW2ol9GmhD9S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Understanding Logistic Regression**\n",
        "# Pre-requisite: **`Linear Regression`**\n",
        "\n",
        "This article discusses the basics of Logistic Regression and its implementation in Python. Logistic regression is basically a supervised classification algorithm. In a classification problem, the target variable(or output), y, can take only discrete values for given set of features(or inputs), X.\n",
        "We can also say that the target variable is categorical. Based on the number of categories, Logistic regression can be classified as:\n",
        "\n",
        "\n",
        "\n",
        "1.   ***binomial:*** target variable can have only 2 possible types: “0” or “1” which may represent “win” vs “loss”, “pass” vs “fail”, “dead” vs “alive”, etc.\n",
        "2.   ***multinomial***: target variable can have 3 or more possible types which are not ordered(i.e. types have no quantitative significance) like “disease A” vs “disease B” vs “disease C”.\n",
        "3. ***ordinal***: it deals with target variables with ordered categories. For example, a test score can be categorized as:“very poor”, “poor”, “good”, “very good”. Here, each category can be given a score like 0, 1, 2, 3.\n",
        "First of all, we explore the simplest form of Logistic Regression, i.e Binomial Logistic Regression.\n",
        "\n",
        "**Binomial Logistic Regression**\n",
        "\n",
        "Consider an example dataset which maps the number of hours of study with the result of an exam. The result can take only two values, namely passed(1) or failed(0):\n",
        "<pre><table>\n",
        "<tr>\n",
        "<th>Hours(x)</th>\n",
        "<td>0.50</td>\n",
        "<td>0.75</td>\n",
        "<td>1.00</td>\n",
        "<td>1.25</td>\n",
        "<td>1.50</td>\n",
        "<td>1.75</td>\n",
        "<td>2.00</td>\n",
        "<td>2.25</td>\n",
        "<td>2.50</td>\n",
        "<td>2.75</td>\n",
        "<td>3.00</td>\n",
        "<td>3.25</td>\n",
        "<td>3.50</td>\n",
        "<td>3.75</td>\n",
        "<td>4.00</td>\n",
        "<td>4.25</td>\n",
        "<td>4.50</td>\n",
        "<td>4.75</td>\n",
        "<td>5.00</td>\n",
        "<td>5.50</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<th>Pass(y)</th>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>0</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "<td>1</td>\n",
        "</tr>\n",
        "</table></pre>\n",
        "So, we have:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?y%20%3D%20%5Cleft%5C%7B%5Cbegin%7Bmatrix%7D%200%2Cif%20fail%5C%5C%201%2Cif%20pass%5C%5C%20%5Cend%7Bmatrix%7D%5Cright.)\n",
        "\n",
        "i.e. y is a categorical target variable which can take only two possible type:“0” or “1”.\n",
        "In order to generalize our model, we assume that:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*  The dataset has ‘p’ feature variables and ‘n’ observations.\n",
        "\n",
        "\n",
        "\n",
        "* The feature matrix is represented as:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?%5Cmathbf%7BX%7D%20%3D%5Cbegin%7Bpmatrix%7D%201%20%26%20x_%7B11%7D%20%26%20%5Ccdots%20%26%20x_%7B1p%7D%20%5C%5C%201%20%26%20x_%7B21%7D%20%26%20%5Ccdots%20%26%20x_%7B2p%7D%20%5C%5C%20%5Cvdots%20%26%20%5Cvdots%20%26%20%5Cddots%20%26%20%5Cvdots%20%5C%5C%201%20%26%20x_%7Bn1%7D%20%26%20%5Ccdots%20%26%20x_%7Bnp%7D%20%5Cend%7Bpmatrix%7D)\n",
        "\n",
        "Here,  x_{ij} denotes the values of  j^{th} feature for  i^{th} observation.\n",
        "Here, we are keeping the convention of letting  x_{i0} = 1. (Keep reading, you will understand the logic in a few moments).\n",
        "* The i^{th} observation, x_i, can be represented as:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?x_i%20%3D%20%5Cbegin%7Bbmatrix%7D%201%5C%5C%20x_%7Bi1%7D%5C%5C%20x_%7Bi2%7D%5C%5C%20.%5C%5C%20.%5C%5C%20x_%7Bip%7D%5C%5C%20%5Cend%7Bbmatrix%7D)\n",
        "\n",
        "* h(x_i) represents the predicted response for  i^{th} observation, i.e.  x_i. The formula we use for calculating  h(x_i) is called **hypothesis**.\n",
        "\n",
        "\n",
        "If you have gone though Linear Regression, you should recall that in Linear Regression, the hypothesis we used for prediction was:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?h%28x_i%29%20%3D%20%5Cbeta_0%20+%20%5Cbeta_1x_%7Bi1%7D%20+%20%5Cbeta_2x_%7Bi2%7D%20+%20.....%20+%20%5Cbeta_px_%7Bip%7D)\n",
        "\n",
        "where,  ***B_0, B_1,…, B_p*** are the regression coefficients.\n",
        "Let regression coefficient matrix/vector,  ***B\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?%5Cbeta%20%3D%20%5Cbegin%7Bbmatrix%7D%20%5Cbeta_0%5C%5C%20%5Cbeta_1%5C%5C%20%5Cbeta_2%5C%5C%20.%5C%5C%20.%5C%5C%20%5Cbeta_p%5C%5C%20%5Cend%7Bbmatrix%7D)\n",
        "\n",
        "\n",
        "Then, in a more compact form,\n",
        "\n",
        "***The reason for taking  x_0 = 1 is pretty clear now. We needed to do a matrix product, but there was no actual  x_0 multiplied to  B_0 in original hypothesis formula. So, we defined  x_0 = 1.***\n",
        "\n",
        "Now, if we try to apply Linear Regression on above problem, we are likely to get continuous values using the hypothesis we discussed above. Also, it does not make sense for  h(x_i) to take values larger that 1 or smaller than 0.\n",
        "\n",
        "So, some modifications are made to the hypothesis for classification:\n",
        "\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?h%28x_i%29%20%3D%20g%28%5Cbeta%5ET%20x_i%29%20%3D%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-%5Cbeta%5ET%20x_i%7D%7D)\n",
        "\n",
        "where,\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B1%20+%20e%5E%7B-z%7D%7D)\n",
        "\n",
        "is called **logistic function** or the sigmoid function.\n",
        "Here is a plot showing g(z):\n",
        "\n",
        "![alt text](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/logistic-function.png)\n",
        "\n",
        "We can infer from above graph that:\n",
        "\n",
        "* g(z) tends towards 1 as  z --> infinity \n",
        "* g(z) tends towards 0 as  z--> infinity\n",
        "* g(z) is always bounded between 0 and 1\n",
        "\n",
        "So, now, we can define conditional probabilities for 2 labels(0 and 1) for  i^{th} observation as:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?%5Cnewline%20P%28y_i%20%3D%201%7Cx_i%3B%20%5Cbeta%29%20%3D%20h%28x_i%29%20%5Cnewline%20P%28y_i%3D0%7Cx_i%3B%20%5Cbeta%29%20%3D%201%20-%20h%28x_i%29)\n",
        "\n",
        "We can write it more compactly as:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?P%28y_i%7Cx_i%3B%5Cbeta%29%20%3D%20%28h%28x_i%29%29%5E%7By_i%7D%281-h%28x_i%29%29%5E%7B1-y_i%7D)\n",
        "\n",
        "Now, we define another term,** likelihood of parameters** as:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?%5Cnewline%20L%28%5Cbeta%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28y_i%7Cx_i%3B%5Cbeta%29%20%5Cnewline%20or%20%5Cnewline%20L%28%5Cbeta%29%20%3D%20%5Cprod_%7Bi%3D1%7D%5E%7Bn%7D%28h%28x_i%29%29%5E%7By_i%7D%281-h%28x_i%29%29%5E%7B1-y_i%7D)\n",
        "\n",
        "Likelihood is nothing but the probability of data(training examples), given a model and specific parameter values(here,  **B**). It measures the support provided by the data for each possible value of the  **B**. We obtain it by multiplying all  P(y_i|x_i) for given  **B**.\n",
        "\n",
        "And for easier calculations, we take **log likelihood**:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?%5Cnewline%20l%28%5Cbeta%29%20%3D%20log%28L%28%5Cbeta%29%29%20%5Cnewline%20or%20%5Cnewline%20l%28%5Cbeta%29%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dy_ilog%28h%28x_i%29%29%20+%20%281-y_i%29log%281-h%28x_i%29%29)\n",
        "\n",
        "The cost function for logistic regression is proportional to inverse of likelihood of parameters. Hence, we can obtain an expression for cost function, J using log likelihood equation as:\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?J%28%5Cbeta%29%20%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20-%20y_ilog%28h%28x_i%29%29%20-%20%281-y_i%29log%281-h%28x_i%29%29)\n",
        "\n",
        "and our aim is to estimate  **B**  so that cost function is minimized !!\n",
        "\n",
        "Using Gradient descent algorithm\n",
        "\n",
        "Firstly, we take partial derivatives of  J(**B**) w.r.t each  **B**_j  subset of  **B** to derive the stochastic gradient descent rule(we present only the final derived value here):\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?%5Cfrac%7B%5Cpartial%20J%28%5Cbeta%29%7D%7B%5Cpartial%20%5Cbeta_j%7D%20%3D%20%28h%28x%29%20-%20y%29x_j)\n",
        "\n",
        "Here, y and h(x) represent the response vector and predicted response vector(respectively). Also,  x_j is the vector representing the observation values for  j^{th} feature.\n",
        "Now, in order to get min  J(**B**),\n",
        "\n",
        "![alt text](https://latex.codecogs.com/gif.latex?%5Cnewline%20Repeat%5C%7B%20%5Cnewline%20%5Cbeta_j%20%3A%3D%20%5Cbeta_j%20-%20%5Calpha%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%28h%28x_i%29-y_i%29x_%7Bij%7D%20%5Cnewline%20%28Simultaneously%5Chspace%7B5%7Dupdate%5Chspace%7B5%7Dall%5Chspace%7B5%7D%5Cbeta_j%29%20%5Cnewline%20%5C%7D)\n",
        "\n",
        "where  **alpha**  is called learning rate and needs to be set explicitly.\n",
        "Let us see the python implementation of above technique on a sample dataset (download it from here):\n",
        "\n",
        "2.25 2.50 2.75 3.00 3.25 3.50 3.75 4.00 4.25 4.50 4.75 5.00 5.50\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_9vPjJ_2qn9P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "37de0f2c-b571-406f-bb5c-26421458a5d2"
      },
      "cell_type": "code",
      "source": [
        "import csv \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "\n",
        "def loadCSV(filename): \n",
        "\t''' \n",
        "\tfunction to load dataset \n",
        "\t'''\n",
        "\twith open(filename,\"r\") as csvfile: \n",
        "\t\tlines = csv.reader(csvfile) \n",
        "\t\tdataset = list(lines) \n",
        "\t\tfor i in range(len(dataset)): \n",
        "\t\t\tdataset[i] = [float(x) for x in dataset[i]]\t \n",
        "\treturn np.array(dataset) \n",
        "\n",
        "\n",
        "def normalize(X): \n",
        "\t''' \n",
        "\tfunction to normalize feature matrix, X \n",
        "\t'''\n",
        "\tmins = np.min(X, axis = 0) \n",
        "\tmaxs = np.max(X, axis = 0) \n",
        "\trng = maxs - mins \n",
        "\tnorm_X = 1 - ((maxs - X)/rng) \n",
        "\treturn norm_X \n",
        "\n",
        "\n",
        "def logistic_func(beta, X): \n",
        "\t''' \n",
        "\tlogistic(sigmoid) function \n",
        "\t'''\n",
        "\treturn 1.0/(1 + np.exp(-np.dot(X, beta.T))) \n",
        "\n",
        "\n",
        "def log_gradient(beta, X, y): \n",
        "\t''' \n",
        "\tlogistic gradient function \n",
        "\t'''\n",
        "\tfirst_calc = logistic_func(beta, X) - y.reshape(X.shape[0], -1) \n",
        "\tfinal_calc = np.dot(first_calc.T, X) \n",
        "\treturn final_calc \n",
        "\n",
        "\n",
        "def cost_func(beta, X, y): \n",
        "\t''' \n",
        "\tcost function, J \n",
        "\t'''\n",
        "\tlog_func_v = logistic_func(beta, X) \n",
        "\ty = np.squeeze(y) \n",
        "\tstep1 = y * np.log(log_func_v) \n",
        "\tstep2 = (1 - y) * np.log(1 - log_func_v) \n",
        "\tfinal = -step1 - step2 \n",
        "\treturn np.mean(final) \n",
        "\n",
        "\n",
        "def grad_desc(X, y, beta, lr=.01, converge_change=.001): \n",
        "\t''' \n",
        "\tgradient descent function \n",
        "\t'''\n",
        "\tcost = cost_func(beta, X, y) \n",
        "\tchange_cost = 1\n",
        "\tnum_iter = 1\n",
        "\t\n",
        "\twhile(change_cost > converge_change): \n",
        "\t\told_cost = cost \n",
        "\t\tbeta = beta - (lr * log_gradient(beta, X, y)) \n",
        "\t\tcost = cost_func(beta, X, y) \n",
        "\t\tchange_cost = old_cost - cost \n",
        "\t\tnum_iter += 1\n",
        "\t\n",
        "\treturn beta, num_iter \n",
        "\n",
        "\n",
        "def pred_values(beta, X): \n",
        "\t''' \n",
        "\tfunction to predict labels \n",
        "\t'''\n",
        "\tpred_prob = logistic_func(beta, X) \n",
        "\tpred_value = np.where(pred_prob >= .5, 1, 0) \n",
        "\treturn np.squeeze(pred_value) \n",
        "\n",
        "\n",
        "def plot_reg(X, y, beta): \n",
        "\t''' \n",
        "\tfunction to plot decision boundary \n",
        "\t'''\n",
        "\t# labelled observations \n",
        "\tx_0 = X[np.where(y == 0.0)] \n",
        "\tx_1 = X[np.where(y == 1.0)] \n",
        "\t\n",
        "\t# plotting points with diff color for diff label \n",
        "\tplt.scatter([x_0[:, 1]], [x_0[:, 2]], c='b', label='y = 0') \n",
        "\tplt.scatter([x_1[:, 1]], [x_1[:, 2]], c='r', label='y = 1') \n",
        "\t\n",
        "\t# plotting decision boundary \n",
        "\tx1 = np.arange(0, 1, 0.1) \n",
        "\tx2 = -(beta[0,0] + beta[0,1]*x1)/beta[0,2] \n",
        "\tplt.plot(x1, x2, c='k', label='reg line') \n",
        "\n",
        "\tplt.xlabel('x1') \n",
        "\tplt.ylabel('x2') \n",
        "\tplt.legend() \n",
        "\tplt.show() \n",
        "\t\n",
        "\n",
        "\t\n",
        "if __name__ == \"__main__\": \n",
        "\t# load the dataset \n",
        "\tdataset = loadCSV('dataset1.csv') \n",
        "\t\n",
        "\t# normalizing feature matrix \n",
        "\tX = normalize(dataset[:, :-1]) \n",
        "\t\n",
        "\t# stacking columns wth all ones in feature matrix \n",
        "\tX = np.hstack((np.matrix(np.ones(X.shape[0])).T, X)) \n",
        "\n",
        "\t# response vector \n",
        "\ty = dataset[:, -1] \n",
        "\n",
        "\t# initial beta values \n",
        "\tbeta = np.matrix(np.zeros(X.shape[1])) \n",
        "\n",
        "\t# beta values after running gradient descent \n",
        "\tbeta, num_iter = grad_desc(X, y, beta) \n",
        "\n",
        "\t# estimated beta values and number of iterations \n",
        "\tprint(\"Estimated regression coefficients:\", beta) \n",
        "\tprint(\"No. of iterations:\", num_iter) \n",
        "\n",
        "\t# predicted labels \n",
        "\ty_pred = pred_values(beta, X) \n",
        "\t\n",
        "\t# number of correctly predicted labels \n",
        "\tprint(\"Correctly predicted labels:\", np.sum(y == y_pred)) \n",
        "\t\n",
        "\t# plotting regression line \n",
        "\tplot_reg(X, y, beta) \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a9650c9cd573>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset1.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# normalizing feature matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a9650c9cd573>\u001b[0m in \u001b[0;36mloadCSV\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mfunction\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \t'''\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset1.csv'"
          ]
        }
      ]
    }
  ]
}